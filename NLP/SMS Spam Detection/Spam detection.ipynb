{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam classifier : UCI dataset\n",
    "# Techniques        : Stemming, Lemmatization, BOW & TF-IDF\n",
    "# Model                 : Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading data, renaming column names :\n",
    "data = pd.read_csv('SMSSpamCollection',sep='\\t',names=[\"label\",\"message\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                 message\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4825   4516                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we see that there are 5572 rows\n",
    "# lets find the no. of spam and ham in the dataset\n",
    "data.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature engineering\n",
    "# lets see more details on the feature\n",
    "# counting no. of words in each sentence - length of sentence\n",
    "\n",
    "data['length'] = data['message'].apply(len)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xc9176d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD8CAYAAABgmUMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAFUxJREFUeJzt3X+w5XV93/Hny0VATHX5cTFkl+2FuEOkTix0gxjTloo/+GFY04EW6pSt3WbbCakY0tFFMyFNxhmYWEEmKXUVFKwFEY1sgYRuEON0piCLGn4TboDCFZS1i5CIBtF3/zif6x527+6e7+Wee+6P52PmzP1+39/Pud/3/XLgxffH+X5TVUiSNKiXjboBSdLCYnBIkjoxOCRJnRgckqRODA5JUicGhySpE4NDktSJwSFJ6sTgkCR1ss+oGxiGQw45pMbHx0fdhiQtKHfeeed3q2psb+MWZXCMj4+zdevWUbchSQtKkv87yDgPVUmSOjE4JEmdGBySpE4MDklSJwaHJKkTg0OS1InBIUnqxOCQJHVicEiSOlmU3xwflvGNN05bf/TCU+e4E0kaHfc4JEmdGBySpE4MDklSJ0MLjiRXJHkqyT3TLPtPSSrJIW0+SS5NMpHkriTH9o1dl+Sh9lo3rH4lSYMZ5h7Hp4GTdi4mORx4G/BYX/lkYHV7bQAua2MPAi4A3ggcB1yQ5MAh9ixJ2ouhBUdVfRXYPs2ii4H3A9VXWwtcVT23AcuTHAa8A9hSVdur6mlgC9OEkSRp7szpOY4kpwHfqqq/3GnRCuDxvvnJVttdfbrfvSHJ1iRbt23bNotdS5L6zVlwJDkA+BDwu9MtnqZWe6jvWqzaVFVrqmrN2Nhen3woSZqhudzj+HngCOAvkzwKrAS+nuRn6e1JHN43diXwxB7qkqQRmbPgqKq7q+rQqhqvqnF6oXBsVX0b2Ayc3a6uOh54pqqeBG4G3p7kwHZS/O2tJkkakWFejns18H+Ao5JMJlm/h+E3AQ8DE8AngN8AqKrtwB8Ad7TX77eaJGlEhnavqqo6ay/Lx/umCzhnN+OuAK6Y1eYkSTPmN8clSZ0YHJKkTgwOSVInBockqRODQ5LUicEhSerE4JAkdWJwSJI6MTgkSZ0YHJKkTgwOSVInBockqRODQ5LUicEhSerE4JAkdWJwSJI6MTgkSZ0YHJKkTgwOSVInQwuOJFckeSrJPX21P0zyQJK7kvxJkuV9y85PMpHkwSTv6Kuf1GoTSTYOq19J0mCGucfxaeCknWpbgNdX1S8CfwWcD5DkaOBM4B+09/zXJMuSLAP+GDgZOBo4q42VJI3I0IKjqr4KbN+p9r+q6oU2exuwsk2vBa6pqr+rqkeACeC49pqoqoer6nngmjZWkjQiozzH8W+BP23TK4DH+5ZNttru6pKkERlJcCT5EPAC8Nmp0jTDag/16X7nhiRbk2zdtm3b7DQqSdrFnAdHknXAO4F3V9VUCEwCh/cNWwk8sYf6LqpqU1Wtqao1Y2Njs9+4JAmY4+BIchLwAeC0qnqub9Fm4Mwk+yU5AlgNfA24A1id5Igk+9I7gb55LnuWJL3YPsP6xUmuBk4ADkkyCVxA7yqq/YAtSQBuq6r/UFX3JrkWuI/eIaxzqurH7ff8JnAzsAy4oqruHVbPkqS9G1pwVNVZ05Qv38P4DwMfnqZ+E3DTLLYmSXoJ/Oa4JKkTg0OS1InBIUnqxOCQJHVicEiSOjE4JEmdGBySpE4MDklSJwaHJKkTg0OS1InBIUnqxOCQJHVicEiSOjE4JEmdGBySpE4MDklSJwaHJKkTg0OS1InBIUnqxOCQJHUytOBIckWSp5Lc01c7KMmWJA+1nwe2epJcmmQiyV1Jju17z7o2/qEk64bVryRpMMPc4/g0cNJOtY3ALVW1GrilzQOcDKxurw3AZdALGuAC4I3AccAFU2EjSRqNoQVHVX0V2L5TeS1wZZu+EnhXX/2q6rkNWJ7kMOAdwJaq2l5VTwNb2DWMJElzaJ85Xt9rqupJgKp6Msmhrb4CeLxv3GSr7a6+iyQb6O2tsGrVqllue8/GN944bf3RC0+d0z4kaS7Ml5PjmaZWe6jvWqzaVFVrqmrN2NjYrDYnSdphroPjO+0QFO3nU60+CRzeN24l8MQe6pKkEZnr4NgMTF0ZtQ64vq9+dru66njgmXZI62bg7UkObCfF395qkqQRGdo5jiRXAycAhySZpHd11IXAtUnWA48BZ7ThNwGnABPAc8B7AKpqe5I/AO5o436/qnY+4S5JmkNDC46qOms3i06cZmwB5+zm91wBXDGLrUmSXoL5cnJckrRAGBySpE4MDklSJwaHJKmTgYIjyeuH3YgkaWEYdI/jvyX5WpLfSLJ8qB1Jkua1gYKjqn4FeDe9b3FvTfI/krxtqJ1Jkualgc9xVNVDwO8AHwD+KXBpkgeS/PNhNSdJmn8GPcfxi0kuBu4H3gL8alW9rk1fPMT+JEnzzKDfHP8j4BPAB6vqB1PFqnoiye8MpTNJ0rw0aHCcAvygqn4MkORlwP5V9VxVfWZo3UmS5p1Bz3H8OfCKvvkDWk2StMQMGhz7V9XfTs206QOG05IkaT4bNDi+n+TYqZkk/wj4wR7GS5IWqUHPcbwP+HySqafvHQb8y+G0JEmazwYKjqq6I8kvAEfRew74A1X1o6F2Jkmal7o8yOmXgPH2nmOSUFVXDaUrSdK8NVBwJPkM8PPAN4Eft3IBBockLTGD7nGsAY5uj3iVJC1hg15VdQ/ws7O10iS/leTeJPckuTrJ/kmOSHJ7koeSfC7Jvm3sfm1+oi0fn60+JEndDRochwD3Jbk5yeap10xWmGQF8F5gTVW9HlgGnAlcBFxcVauBp4H17S3rgaer6rX07ot10UzWK0maHYMeqvq9Iaz3FUl+RO+LhE/Su2Hiv2rLr2zrvAxY27f+64A/ShIPm0nSaAz6PI6/AB4FXt6m7wC+PpMVVtW3gI8Aj9ELjGeAO4HvVdULbdgksKJNrwAeb+99oY0/eCbrliS9dIPeVv3X6f3f/sdbaQXwpZmsMMmB9PYijgB+DnglcPI0Q6f2KLKHZf2/d0OSrUm2btu2bSatSZIGMOg5jnOANwPPwk8f6nToDNf5VuCRqtrWvkT4ReCXgeVJpg6drQSmvqU+Se/Jg7Tlrwa27/xLq2pTVa2pqjVjY2MzbE2StDeDBsffVdXzUzPtP+AzPcfwGHB8kgOSBDgRuA+4FTi9jVkHXN+mN7d52vIve35DkkZn0OD4iyQfpHdC+23A54H/OZMVVtXt9A57fR24u/Wwid4jac9LMkHvHMbl7S2XAwe3+nnAxpmsV5I0Owa9qmojvcti7wb+PXAT8MmZrrSqLgAu2Kn8MHDcNGN/CJwx03VJkmbXoDc5/Am9R8d+YrjtSJLmu0HvVfUI05zTqKojZ70jSdK81uVeVVP2p3fo6KDZb0eSNN8N+gXA/9f3+lZVXULvm96SpCVm0ENVx/bNvozeHsjfG0pHkqR5bdBDVf+lb/oFercf+Rez3o0kad4b9KqqfzbsRiRJC8Ogh6rO29Pyqvro7LQjSZrvulxV9Uv0bv8B8KvAV2l3rZUkLR2DBschwLFV9TcASX4P+HxV/bthNSZJmp8GvVfVKuD5vvnngfFZ70aSNO8NusfxGeBrSf6E3jfIfw24amhdSZLmrUGvqvpwkj8F/nErvaeqvjG8tiRJ89Wgh6qg92zwZ6vqY8BkkiOG1JMkaR4b9HLcC+hdWXUU8Cng5cB/p/dUQM0z4xtvnLb+6IWnznEnkhajQfc4fg04Dfg+QFU9gbcckaQladDgeL49rrUAkrxyeC1JkuazQYPj2iQfB5Yn+XXgz/GhTpK0JA16VdVH2rPGn6V3nuN3q2rLUDuTJM1Lew2OJMuAm6vqrYBhIUlL3F4PVVXVj4Hnkrx6tlaaZHmS65I8kOT+JG9KclCSLUkeaj8PbGOT5NIkE0nu2unZIJKkOTboN8d/CNydZAvtyiqAqnrvDNf7MeDPqur0JPvS+47IB4FbqurCJBuBjcAHgJOB1e31RuCy9nPe87JYSYvRoMFxY3u9ZEleBfwT4N8AVNXzwPNJ1gIntGFXAl+hFxxrgavaVV23tb2Vw6rqydnoR5LUzR6DI8mqqnqsqq6cxXUeCWwDPpXkDcCdwLnAa6bCoKqeTHJoG7+CF9++fbLVXhQcSTYAGwBWrVo1i+1Kkvrt7RzHl6Ymknxhlta5D3AscFlVHUPv0NfGPYzPNLXapVC1qarWVNWasbGx2elUkrSLvQVH/3+0j5yldU4Ck1V1e5u/jl6QfCfJYQDt51N94w/ve/9K4IlZ6kWS1NHegqN2Mz1jVfVt4PEkR7XSicB99J4uuK7V1gHXt+nNwNnt6qrjgWc8vyFJo7O3k+NvSPIsvT2PV7Rp2nxV1atmuN7/CHy2XVH1MPAeeiF2bZL1wGPAGW3sTcApwATwXBsrSRqRPQZHVS0bxkqr6pv07ra7sxOnGVvAOcPoQ5LUXZfncUiSZHBIkroxOCRJnRgckqRODA5JUicGhySpE4NDktSJwSFJ6sTgkCR1YnBIkjoxOCRJnRgckqRODA5JUicGhySpE4NDktSJwSFJ6mRvTwDUEIxvvHHa+qMXnjrHnUhSd+5xSJI6MTgkSZ2MLDiSLEvyjSQ3tPkjktye5KEkn0uyb6vv1+Yn2vLxUfUsSRrtHse5wP198xcBF1fVauBpYH2rrweerqrXAhe3cZKkERlJcCRZCZwKfLLNB3gLcF0bciXwrja9ts3Tlp/YxkuSRmBUexyXAO8HftLmDwa+V1UvtPlJYEWbXgE8DtCWP9PGS5JGYM4vx03yTuCpqrozyQlT5WmG1gDL+n/vBmADwKpVq2ah0/lvd5f1StIwjWKP483AaUkeBa6hd4jqEmB5kqkgWwk80aYngcMB2vJXA9t3/qVVtamq1lTVmrGxseH+BZK0hM15cFTV+VW1sqrGgTOBL1fVu4FbgdPbsHXA9W16c5unLf9yVe2yxyFJmhvz6XscHwDOSzJB7xzG5a1+OXBwq58HbBxRf5IkRnzLkar6CvCVNv0wcNw0Y34InDGnjUmSdms+7XFIkhYAg0OS1InBIUnqxOCQJHVicEiSOjE4JEmd+ATABcBbi0iaT9zjkCR1YnBIkjrxUNU84iEpSQuBexySpE4MDklSJwaHJKkTg0OS1InBIUnqxOCQJHVicEiSOjE4JEmdGBySpE4MDklSJ3MeHEkOT3JrkvuT3Jvk3FY/KMmWJA+1nwe2epJcmmQiyV1Jjp3rniVJO4xij+MF4Ler6nXA8cA5SY4GNgK3VNVq4JY2D3AysLq9NgCXzX3LkqQpcx4cVfVkVX29Tf8NcD+wAlgLXNmGXQm8q02vBa6qntuA5UkOm+O2JUnNSM9xJBkHjgFuB15TVU9CL1yAQ9uwFcDjfW+bbLWdf9eGJFuTbN22bdsw25akJW1kwZHkZ4AvAO+rqmf3NHSaWu1SqNpUVWuqas3Y2NhstSlJ2slIgiPJy+mFxmer6out/J2pQ1Dt51OtPgkc3vf2lcATc9WrJOnFRnFVVYDLgfur6qN9izYD69r0OuD6vvrZ7eqq44Fnpg5pSZLm3iieAPhm4F8Ddyf5Zqt9ELgQuDbJeuAx4Iy27CbgFGACeA54z9y2K0nqN+fBUVX/m+nPWwCcOM34As4ZalOSpIH5zPFp+OxvSdo9bzkiSerE4JAkdWJwSJI6MTgkSZ0YHJKkTgwOSVInBockqRODQ5LUicEhSerE4JAkdWJwSJI68V5VS8ie7sH16IWnzmEnkhYy9zgkSZ0YHJKkTgwOSVInBockqRODQ5LUicEhSepkwVyOm+Qk4GPAMuCTVXXhiFtaVHZ3qa6X6Ura2YIIjiTLgD8G3gZMAnck2VxV9422s6XLoJGWrgURHMBxwERVPQyQ5BpgLWBwDNmevjTYZfzuAsUAkhaehRIcK4DH++YngTeOqBfNwGwFUFddA2tP7+nKUNRitVCCI9PU6kUDkg3Ahjb7t0kenOG6DgG+O8P3LjYLflvkoll7z6xti5n0NM8s+M/FLFps2+LvDzJooQTHJHB43/xK4In+AVW1Cdj0UleUZGtVrXmpv2cxcFvs4LbYwW2xw1LdFgvlctw7gNVJjkiyL3AmsHnEPUnSkrQg9jiq6oUkvwncTO9y3Cuq6t4RtyVJS9KCCA6AqroJuGkOVvWSD3ctIm6LHdwWO7gtdliS2yJVtfdRkiQ1C+UchyRpnjA4miQnJXkwyUSSjaPuZ9iSHJ7k1iT3J7k3ybmtflCSLUkeaj8PbPUkubRtn7uSHDvav2D2JVmW5BtJbmjzRyS5vW2Lz7ULM0iyX5ufaMvHR9n3bEuyPMl1SR5on483LdXPRZLfav9+3JPk6iT7L9XPRT+Dgxfd0uRk4GjgrCRHj7aroXsB+O2qeh1wPHBO+5s3ArdU1WrgljYPvW2zur02AJfNfctDdy5wf9/8RcDFbVs8Daxv9fXA01X1WuDiNm4x+RjwZ1X1C8Ab6G2TJfe5SLICeC+wpqpeT+/CnDNZup+LHapqyb+ANwE3982fD5w/6r7meBtcT+9eYA8Ch7XaYcCDbfrjwFl94386bjG86H036BbgLcAN9L50+l1gn50/I/Su7ntTm96njcuo/4ZZ2g6vAh7Z+e9Zip8Ldtyx4qD2z/kG4B1L8XOx88s9jp7pbmmyYkS9zLm2S30McDvwmqp6EqD9PLQNW+zb6BLg/cBP2vzBwPeq6oU23//3/nRbtOXPtPGLwZHANuBT7bDdJ5O8kiX4uaiqbwEfAR4DnqT3z/lOlubn4kUMjp693tJksUryM8AXgPdV1bN7GjpNbVFsoyTvBJ6qqjv7y9MMrQGWLXT7AMcCl1XVMcD32XFYajqLdlu08zhrgSOAnwNeSe/Q3M6WwufiRQyOnr3e0mQxSvJyeqHx2ar6Yit/J8lhbflhwFOtvpi30ZuB05I8ClxD73DVJcDyJFPfder/e3+6LdryVwPb57LhIZoEJqvq9jZ/Hb0gWYqfi7cCj1TVtqr6EfBF4JdZmp+LFzE4epbcLU2SBLgcuL+qPtq3aDOwrk2vo3fuY6p+druK5njgmalDFwtdVZ1fVSurapzeP/svV9W7gVuB09uwnbfF1DY6vY1fFP9nWVXfBh5PclQrnUjv8QVL7nNB7xDV8UkOaP++TG2LJfe52MWoT7LMlxdwCvBXwF8DHxp1P3Pw9/4Kvd3ou4Bvttcp9I7J3gI81H4e1MaH3pVnfw3cTe9Kk5H/HUPYLicAN7TpI4GvARPA54H9Wn3/Nj/Rlh856r5neRv8Q2Br+2x8CThwqX4ugP8MPADcA3wG2G+pfi76X35zXJLUiYeqJEmdGBySpE4MDklSJwaHJKkTg0OS1InBIUnqxOCQJHVicEiSOvn/eMECVqu04fUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualizing the length of words in sentences for better clarity\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "data['length'].plot(kind='hist',bins=50)\n",
    "# we see that that length peaks to high values as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5572.000000\n",
       "mean       80.616296\n",
       "std        60.015593\n",
       "min         2.000000\n",
       "25%        36.000000\n",
       "50%        62.000000\n",
       "75%       122.000000\n",
       "max       910.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['length'].describe()\n",
    "# we see that maximum & minimum count of words in a sentence was 910."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1085    For me the love should start with attraction.i...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets try to extract thost messages\n",
    "# maximum count\n",
    "data.loc[data['length']==910,'message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1925    Ok\n",
       "3051    Ok\n",
       "4498    Ok\n",
       "5357    Ok\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# minimum count\n",
    "data.loc[data['length']==2,'message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning steps :\n",
    "# Remove all unnecessary characters like punctuations,special characters & numbers.\n",
    "# Convert all words to lower case so that it doesnt get recognised as a seperate character.\n",
    "# Split into words.\n",
    "# Check the words and do stemming process if the word is not in stopwords.\n",
    "# Finalize the list of words into label \"corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "corpus=[]\n",
    "for i in range(0,len(data)) :\n",
    "    review = re.sub('[^a-zA-Z]',' ',data['message'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization : BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# has 6296 words, however minimizing to 2500 top words used  in the dataset\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=2500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2500)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('message :', 'U dun say so early hor... U c already then say...')\n",
      "  (0, 623)\t1\n",
      "  (0, 1836)\t2\n",
      "('shape of the matrix is :', (1, 2500))\n",
      "('Word which appeared once :', u'dun')\n",
      "('Word which appeared twice :', u'say')\n"
     ]
    }
   ],
   "source": [
    "# Analysis of vectorized data, (eg).lets check for the message 3\n",
    "message4 = data['message'][3]\n",
    "print(\"message :\",message4)\n",
    "bow4 = cv.transform([message4])\n",
    "print(bow4)\n",
    "print(\"shape of the matrix is :\",bow4.shape)\n",
    "\n",
    "# This means that there are 2 unique words in message4(Row 623 & Row 1836), out of which 1 word appears twice and 1 word appears once.\n",
    "# Lets check what those words are :\n",
    "\n",
    "print(\"Word which appeared once :\",cv.get_feature_names()[623])\n",
    "print(\"Word which appeared twice :\",cv.get_feature_names()[1836])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simplifying the putput value - 'label'\n",
    "# lets take ham as 0 and spam as 1 \n",
    "\n",
    "y=pd.get_dummies(data['label'])\n",
    "y=y.iloc[:,1].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelling\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB().fit(X_train,y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[946,   9],\n",
       "       [  7, 153]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix:\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9856502242152466"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization : TF - IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2500)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv1 = TfidfVectorizer(max_features=2500)\n",
    "X1 = cv1.fit_transform(corpus).toarray()\n",
    "\n",
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelling\n",
    "from sklearn.model_selection import train_test_split\n",
    "X1_train,X1_test,y_train,y_test = train_test_split(X1,y,test_size=0.20,random_state=0)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model1 = MultinomialNB().fit(X1_train,y_train)\n",
    "\n",
    "y_pred = model.predict(X1_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[953,   2],\n",
       "       [ 17, 143]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9829596412556054"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see that the accuracy is better with vectorization : Bag of words.\n",
    "# However here we have followed stemming process for text processing, lets check for any difference in accuracy with lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPAM classifier using lemmatization & BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lm = WordNetLemmatizer()\n",
    "corpus1 = []\n",
    "for i in range(0,len(data)) :\n",
    "    reviewlem = re.sub('[^a-zA-Z]',' ',data['message'][i])\n",
    "    reviewlem = review.lower()\n",
    "    reviewlem = review.split()\n",
    "\n",
    "    reviewlem = [lm.lemmatize(word) for word in reviewlem if not word in set(stopwords.words('english'))]\n",
    "    reviewlem = ' '.join(reviewlem)\n",
    "    corpus1.append(reviewlem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [1, 1, 1],\n",
       "       [1, 1, 1],\n",
       "       ...,\n",
       "       [1, 1, 1],\n",
       "       [1, 1, 1],\n",
       "       [1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv2 = CountVectorizer(max_features=2500)\n",
    "X2 = cv2.fit_transform(corpus1).toarray()\n",
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X2_train,X2_test,y_train,y_test = train_test_split(X2,y,test_size=0.20,random_state=0)\n",
    "# modelling\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model2 = MultinomialNB().fit(X2_train,y_train)\n",
    "\n",
    "y_pred = model2.predict(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[955, 160],\n",
       "       [  0,   0]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "confusion_matrix(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8565022421524664"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score\n",
    "accuracy_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The accuracy score through stemming : 98.5%\n",
    "# The accuracy score through lemmatization : 85.6%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
